---
##############
# Test config
#
# Detailed information on the parameters are given in the SetupManager
# class located in dnn_reco/setup_manager.py.
##############

# Provide a unique name for the model
'unique_name': 'numu_11883_bigger_quality'

#---------------------------
# General settings
#---------------------------
'training_data_file' : [
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/01000-01999/*.hdf5',
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/02000-02999/*.hdf5',
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/03000-03999/*.hdf5',
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/04000-04999/*.hdf5',
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/05000-05999/*.hdf5',
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/06000-06999/*.hdf5',
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/07000-07999/*.hdf5',
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/08000-08999/*.hdf5',
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/09000-09999/*.hdf5',
  ]
'trafo_data_file' : [
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/01000-01999/*.hdf5',
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/02000-02999/*.hdf5',
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/03000-03999/*.hdf5',
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/04000-04999/*.hdf5',
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/05000-05999/*.hdf5',
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/06000-06999/*.hdf5',
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/07000-07999/*.hdf5',
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/08000-08999/*.hdf5',
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/09000-09999/*.hdf5',
  ]
'validation_data_file' : [
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/00000-00999/*.hdf5',
  ]
'test_data_file' : [
    '/net/big-tank/POOL/users/mhuennefeld/data/dnn_reco/NuMu/11883/datasets/11883/clsim-base-4.0.5*/output/summaryV2_clipped/00000-00999/*.hdf5',
  ]

'tf_random_seed': 42
'float_precision': float32
'num_jobs' : 15
'file_capacity' : 20
'batch_capacity' : 200
'num_add_files' : 10
'num_repetitions' : 3
#'DOM_init_values' : [[[[[0., -0.020098502, -0.014917467, 0.057788417, 0.03707111, 0., 0.]]]]]
'batch_size' : 32

'log_path' : "../logs/\
                {unique_name}/\
                {model_file}__\
                {model_name}"

#----------------------
# Data Handler settings
#----------------------
# Name of data bin values key (e.g. DOMPulseBinValues, dnn_data_bin_values)
'data_handler_bin_values_name': dnn_data_bin_values
# Name of data bin indices key (e.g. DOMPulseBinIndices, dnn_data_bin_indices)
'data_handler_bin_indices_name': dnn_data_bin_indices
# Name of data global time offset key (e.g. DOMPulseTimeRangeStart, dnn_data_global_time_offset)
'data_handler_time_offset_name': dnn_data_global_time_offset

'data_handler_num_bins': 9

'data_handler_label_file': 'default_labels'
'data_handler_label_name': 'simple_label_loader'
'data_handler_misc_file': 'default_misc'
'data_handler_misc_name': 'general_misc_loader'
'data_handler_filter_file': 'default_filter'
'data_handler_filter_name': 'general_filter'


'data_handler_label_key': 'LabelsDeepLearning'


# must be a list of keys or empty list
'data_handler_relative_time_keys': []
# lower case pattern
'data_handler_relative_time_key_pattern': 'time'

# --------------
# Misc settings
# --------------
# The general_misc_loader will load the keys defined in the misc_load_dict
# from the training files. The pattern is: 'hdf key': 'column'
# These values will then be added to the misc values under the name:
#     'hdf key'_'column'
'misc_load_dict': {
  'LabelsDeepLearning': 'LengthInDetector',
  'CorsikaWeightMap': 'Multiplicity',
}

# --------------
# Filter settings
# --------------
# The general_filter will filter events according to the key value pairs
# defined in the dicts filter_equal, filter_greater_than, filter_less_than
# The keys used here must exist in the loaded misc names.

# For events to pass filter, the following must be True: misc[key] == value
'filter_equal': {
  'CorsikaWeightMap_Multiplicity': 1,
}
# For events to pass filter, the following must be True: misc[key] > value
'filter_greater_than': {
  'LabelsDeepLearning_LengthInDetector': 50,
}
# For events to pass filter, the following must be True: misc[key] < value
'filter_less_than': {
}

# --------------
# Label settings
# --------------
# initialize label weights with this value
'label_weight_initialization': 0 # 0.0001
# Weights of each label are initialized with label_weight_initialization,
# unless defined otherwise here
'label_weight_dict': {
  # 'EnergyVisible': 0.1,
  # 'PrimaryEnergy': 0.1,
  # 'VertexTime': 0,
  # 'LengthInDetector': 0,
  'PrimaryAzimuth': 0.01,
  'PrimaryZenith': 0.01,
  'PrimaryDirectionX': 3,
  'PrimaryDirectionY': 3,
  'PrimaryDirectionZ': 3,
  # 'VertexX': 0,
  # 'VertexY': 0,
  # 'VertexZ': 0,
  # 'p_starting': 0,
  # 'p_starting_300m': 0,
  # 'p_is_track': 0,
  # 'p_entering': 0,
  # 'p_entering_muon_single': 0,
  # 'p_entering_muon_bundle': 0,
  # 'p_outside_cascade': 0,
}
# Keys to use for I3Particle
'label_particle_keys': {
  'energy': EnergyVisible,
  'time': VertexTime,
  'length': LengthInDetector,
  'dir_x': PrimaryDirectionX,
  'dir_y': PrimaryDirectionY,
  'dir_z': PrimaryDirectionZ,
  'pos_x': VertexX,
  'pos_y': VertexY,
  'pos_z': VertexZ,
}
# Update label weights during training
'label_update_weights': True
# Scale median absolute residuals for tukey loss
'label_scale_tukey': False
# Name of zenith direction label if it exists
'label_zenith_key': PrimaryZenith
# Name of azimuth direction label if it exists
'label_azimuth_key': PrimaryAzimuth
# Name of direction vector x-component label if it exists
'label_dir_x_key': PrimaryDirectionX
# Name of direction vector x-component label if it exists
'label_dir_y_key': PrimaryDirectionY
# Name of direction vector x-component label if it exists
'label_dir_z_key': PrimaryDirectionZ
# Add direction vector components as labels
'label_add_dir_vec': False
# Add position at the provided relative time (relative to time range start)
'label_position_at_rel_time':
# Define pid keys. The labels defined here will be fors to range [0, 1]
# (depends on the chosen neural network model)
'label_pid_keys': [
    'p_cc_e', 'p_cc_mu', 'p_cc_tau', 'p_nc',
    'p_starting', 'p_starting_300m', 'p_starting_glashow',
    'p_starting_nc', 'p_starting_cc', 'p_starting_cc_e',
    'p_starting_cc_mu', 'p_starting_cc_tau',
    'p_starting_cc_tau_muon_decay', 'p_is_track',
    'p_starting_cc_tau_double_bang', 'p_entering',
    'p_entering_muon_single', 'p_entering_muon_bundle',
    'p_outside_cascade', 'p_entering_muon_single_stopping',
]

#---------------------------
# General Training settings
#---------------------------
'num_training_iterations' : 100000000
'validation_frequency' : 100
'save_frequency' : 500
'keep_probability_list' : [0.95, 1.0, 1.0, 1.0]
# A custom evaluation method can be defined here.
# If defined, this method will be run during each validation step.
'evaluation_file': default_evaluation
'evaluation_name': eval_direction

#---------------------------
# Trafo settings
#---------------------------
'trafo_num_jobs' : 25
'trafo_num_batches' : 1000
'trafo_model_path' : '../data/trafo_models/dnn_reco_11883_quality_big_trafo_model.npy'
'trafo_normalize_dom_data' : True
'trafo_normalize_label_data' : True
'trafo_normalize_misc_data' : False
'trafo_log_dom_bins' : [False, False, False, False, False, False, False, False, False]
'trafo_log_label_bins' : {
                            'EnergyVisible': False,
                            'PrimaryEnergy': False,
                            'Length': False,
                         }
'trafo_log_misc_bins' : False
'trafo_treat_doms_equally' : True
'trafo_norm_constant' : 0.0001

#------------------
# NN Model Training
#------------------
'model_checkpoint_path' : "../checkpoints/nn_model/\
                              {model_file}__\
                              {model_name}/\
                              {unique_name}/model"
'model_restore_model' : True
'model_save_model' : True

# Define a dictionary of dictionaries of optimizers here.
# Each optimizer has to define the following fields:
#   'optimizer': name of tf.train.Optimizer, e.g. 'AdamOptimizer'
#   'optimizer_settings': a dictionary of settings for the optimizer
#   'vars': str or list of str specifying the variables the optimizer is
#           adujusting. E.g. ['unc', 'pred'] to optimize weights of the
#           main prediction network and the uncertainty subnetwork.
#   'loss_file': str or list of str, defines file of loss function
#   'loss_name': str or list of str, defines name of loss function
#                If loss_file and loss_name are lists, they must have the same
#                length. In this case, a sum of each loss will be performed
#   'l1_regularization': Regularization strength (lambda) for L1-Regularization
#   'l2_regularization': Regularization strength (lambda) for L2-Regularization
# This structure might seem a bit confusing, but it enables the use of
# different tensorflow optimizer operations, which can each apply to
# different weights of the network and wrt different loss functions.
'model_optimizer_dict': {

  # define an arbitrary name of optimizer here
  'simple_mse': {
                  'optimizer': 'AdamOptimizer',
                  'optimizer_settings': {'learning_rate': 0.001,},
                  'vars' : ['pred', 'unc'],
                  'loss_file': 'default_loss',
                  'loss_name': 'mse',
                  'l1_regularization': 0.,
                  'l2_regularization': 0.00000001,
                },
  # 'gaussian_unc': {
  #                 'optimizer': 'AdamOptimizer',
  #                 'optimizer_settings': {'learning_rate': 0.001,},
  #                 'vars' : ['unc'],
  #                 'loss_file': 'default_loss',
  #                 'loss_name': 'gaussian_likelihood',
  #                 'l1_regularization': 0.,
  #                 'l2_regularization': 0.00001,
  #               },
}

#----------------------
# NN Model Architecture
#----------------------
'model_file' : 'general_IC86_models'
'model_name' : 'general_model_IC86'
'model_is_training' : True

# 2D convolutional layer of upper DeepCore
'conv_upper_DeepCore_settings': {
    'filter_size_list': [[1, 9], [1, 9], [1, 9], [1, 7], [1, 7], [1, 7], [1, 5],
                         [1, 5], [1, 5], [1, 1], [1, 1]],
    'num_filters_list': [150, 150, 150, 150, 150, 150, 150, 150],
    'pooling_type_list': [False, False, 'max', False, False, 'max', False, False],
    'pooling_strides_list': [1, 1, 2, 1],
    'pooling_ksize_list': [1, 1, 2, 1],
    'use_dropout_list': True,
    'padding_list': 'SAME',
    'strides_list': [1, 1, 1, 1],
    'use_batch_normalisation_list': False,
    'activation_list': 'relu',
    'use_residual_list': True,
}

# 2D convolutional layer of lower DeepCore
'conv_lower_DeepCore_settings': {
    'filter_size_list': [[1, 9], [1, 9], [1, 9], [1, 9], [1, 9], [1, 9], [1, 9],
                         [1, 9], [1, 9], [1, 9], [1, 9], [1, 9], [1, 1], [1, 1]],
    'num_filters_list': [150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150,
                         150, 150, 150],
    'pooling_type_list': [False, False, 'max',
                          False, False, 'max',
                          False, False, 'max',
                          False, False, 'max',
                          False, False],
    'pooling_strides_list': [1, 1, 2, 1],
    'pooling_ksize_list': [1, 1, 2, 1],
    'use_dropout_list': True,
    'padding_list': 'SAME',
    'strides_list': [1, 1, 1, 1],
    'use_batch_normalisation_list': False,
    'activation_list': 'relu',
    'use_residual_list': True,
}

# 3D hexagonal convolution over main IceCube array (IC78)
'conv_IC78_settings' : {
    'filter_size_list': [[2, 0, 3], [2, 0, 3], [1, 0, 15], [2, 0, 3], [1, 0, 15],
                         [2, 0, 3], [2, 0, 3], [2, 0, 3], [2, 0, 3], [2, 0, 3],
                         [2, 0, 3], [2, 0, 3], [2, 0, 3], [2, 0, 3], [2, 0, 3],
                         [2, 0, 3], [2, 0, 3], [2, 0, 3], [1, 0, 3], [1, 0, 1]],
    'num_filters_list': [150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150,
                         150, 150, 150, 150, 150, 150, 150],
    'pooling_type_list': [False, False, 'max', False, False, False, False,
                          False,'max', False, False, False, False, False, 'max',
                          False, False, 'max', False, False],
    'pooling_strides_list': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 2, 1],
                             [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1],
                             [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 2, 2, 2, 1],
                             [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1],
                             [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 2, 2, 2, 1],
                             [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 2, 2, 2, 1],
                             [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]],
    'pooling_ksize_list': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 2, 1],
                             [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1],
                             [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 2, 2, 2, 1],
                             [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1],
                             [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 2, 2, 2, 1],
                             [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 2, 2, 2, 1],
                             [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]],
    'use_dropout_list': True,
    'padding_list': 'SAME',
    'strides_list': [1, 1, 1, 1, 1],
    'use_batch_normalisation_list': False,
    'activation_list': 'relu',
    'use_residual_list': True,
    'hex_zero_out_list': False,
    'dilation_rate_list': ,
    'hex_num_rotations_list': 1,
}

# Fully connected layer settings (Combine results from convolutions)
'fc_settings': {
    'fc_sizes': [300, -1], # last one will be overwritten with num labels
    'use_dropout_list': [True, False],
    'activation_list': ['relu', ''],
    'use_batch_normalisation_list': False,
    'use_residual_list': [False, True],
    'max_out_size_list': ,
}

# Fully connected layer settings for uncertainty subnetwork
'fc_unc_settings': {
    'fc_sizes': [300, 300, -1], # last one will be overwritten with num labels
    'use_dropout_list': [True, True, False],
    'activation_list': ['relu', 'relu', 'abs'],
    'use_batch_normalisation_list': False,
    'use_residual_list': [False, True, True],
    'max_out_size_list': ,
}

...
